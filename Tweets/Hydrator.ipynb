{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydrating Twitter IDs!\n",
    "#### More information on the Twitter IDs and the Twarc package can be found in the Sources section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in necessary libraries and Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "from twarc import Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in credentials so I can connect to API\n",
    "t = Twarc('HV2haozYztEHhD38iASch50lL', \n",
    "          'XPaRlEkbZjnK5CLEW6pXmDzdKi5Bs3df4rFW9krmNJTg5YYPri',\n",
    "          '979782709298868224-Jw93XDCp5uzuKsD2kuQABD0QwdwnIb2', \n",
    "          'duMEIGT0m1HhNLicPRS3m3eKgJ7FimvVX85egBYE7PW64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Option 1 | Read in JSON Files and Parse JSON</u>\n",
    "#### I used the command line and twarc to hydrate one text file with about 50k Twitter IDs which resulted in a single JSON file. The code below parses through the JSON file and stores the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The JSON file has thousands of JSON objects in it, so we need to first open the file, \n",
    "# loop through each line, and use the json.loads() function to extract each object\n",
    "tweets = []\n",
    "with open('apriltweets\\\\some_april_tweets.json') as f:\n",
    "    for line in f:\n",
    "        tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets do we have in this JSON file\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the available data look like for one tweet\n",
    "tweets[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing out the first five tweets in the file\n",
    "[print(tweets[i]['full_text'],'\\n\\n') for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a Tweet was retweeted, the text may be shortened. For example, in this tweet below the 'full text'\n",
    "# is actually cut short, but in the retweeted status we can see the full text.\n",
    "print(tweets[27]['user']['location'], tweets[27]['full_text']), tweets[27]['retweeted_status']['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the first five user locations in the file\n",
    "[tweets[i]['user']['location'] for i in range(15)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how many users have 'Seattle' as their exact location and what they tweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweets = pd.Series([tweets[i]['full_text'] for i in range(len(tweets))])\n",
    "locations = pd.Series([tweets[i]['user']['location'] for i in range(len(tweets))])\n",
    "seattle = pd.Series([tweets[i]['user']['location'] == 'Seattle' for i in range(len(tweets))])\n",
    "full_text_tweets = []\n",
    "for i in range(len(tweets)):\n",
    "    try:\n",
    "        full_text_tweets.append(tweets[i]['retweeted_status']['full_text'])\n",
    "    except:\n",
    "        full_text_tweets.append('n/a')\n",
    "full_text_tweets = pd.Series(full_text_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} users who have 'Seattle' as their location\".format(len(locations[seattle])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's be more flexible with the location\n",
    "flexible = pd.Series([tweets[i]['user']['location'] in \n",
    "                      ['Seattle','seattle', 'PNW', 'Seattle, Wa'] for i in range(len(tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} users who have 'Seattle', 'seattle', 'PNW', 'Seattle, Wa' as their location\".format(len(locations[flexible])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in full_text_tweets[seattle]:\n",
    "    print(i,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations are not great, but some users are geo_enabled and have coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_coords = []\n",
    "for i in tweets:\n",
    "    if i['coordinates'] != None:\n",
    "        tweets_coords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 130 Users in this file have a geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the coordinates + location for one tweet \n",
    "tweets_coords[1]['user']['location'], tweets_coords[1]['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all lats and lons and store them in a geopandas df for plotting\n",
    "lats = [tweets_coords[i]['coordinates']['coordinates'][0] for i in range(len(tweets_coords))]\n",
    "lons = [tweets_coords[i]['coordinates']['coordinates'][1] for i in range(len(tweets_coords))]#\n",
    "geometry = [Point(xy) for xy in zip(lats, lons)]\n",
    "geodf = gpd.GeoDataFrame(geometry, columns = ['geometry'])\n",
    "# Check it works\n",
    "geodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the points!\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "geodf.plot(ax=world.plot(figsize=(15,15)), marker = 'o', color = 'red', markersize = 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Option 2 | Read in .txt Files & Hydrate them in a Jupyter Notebook</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, Bring in .txt Files of Twitter IDs and Merging them\n",
    "#### We have 4 folders (corresponding to Jan, Feb, Mar, Apr), each of which contains approx. 400-500 .txt files, and each file holds about 50k Twitter IDs that need to be hydrated. See example below for merging each text file in the April folder, resulting in a list with 405 sublists, each with thousands of Twitter IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for file in os.listdir('apriltweets'):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith( ('.txt') ):\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for file in filenames:\n",
    "    with open('apriltweets\\\\'+file,'r') as tweetids:\n",
    "        ids.append(tweetids.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testids = [ids[0][0:19], ids[0][20:39], ids[0][40:59]]\n",
    "testids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, Hydrate the texts from within Jupyter Notebook - Warning: This goes much slower than using the app_auth in the command line, and rate limits are exceeded much quicker than when using using app_auth: https://github.com/DocNow/twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontweets = []\n",
    "for tweet in t.hydrate(ids[0]):\n",
    "    jsontweets.append(tweet)\n",
    "    \n",
    "#for tweet in t.hydrate(open('ids.txt')):\n",
    "#    print(tweet[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testids = ['1245140084313206786', '1245140084350910464', '1245140084417941505']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontweets = []\n",
    "for i in t.hydrate(testids):\n",
    "    jsontweets.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontweets = json_normalize(jsontweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontweets[['created_at', 'full_text', 'coordinates', 'user.location', 'retweeted_status.full_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Option 3 | Read in .txt Files, merge them, write them to a new text file for hydrating in command line</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "\n",
    "for file in os.listdir('apriltweets'):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith( ('.txt') ):\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for file in filenames:\n",
    "    with open('apriltweets\\\\'+file,'r') as tweetids:\n",
    "        ids.append(tweetids.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these merged ids\n",
    "with open('small_merged_twitter_ids.txt', 'w') as outfile:\n",
    "    for i in ids[0:5]:\n",
    "        outfile.write(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "#### Twarc: https://github.com/DocNow/twarc\n",
    "#### Twitter Ids: https://github.com/echen102/COVID-19-TweetIDs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
